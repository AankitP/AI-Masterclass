# Deep Neuro Evolution

this is a cool technology/methodology that is used in newer ANNs

there are 2 techniques today

- CMA-ES : Covariance Matrix Adaptation Evolution strategy
  - this is used when there aren't that many parameters
- PEPG : Parameter Exploring Policy Gradients
  - This is used when there are alot of parameters (thousands)

these only optimize the parameters of the controller - the others use classical training methods/optimizers (such as the adam optimizer)

The controller is a single layer linear model
$$ a_t = W_c [z_t h_t] + b_c $$

this will be optimized by the evolution strategies technique (EST), meaning

- the EST will find the best parameters within the matrix W<sub>c</sub> to return the best actions to execute, when the latent vector c, from the CNN-VAE, and the hidden state h, from the MDN-RNN, are the inputs
- the goal is to maximize the cumulative reward

ES optimizers are an alternative to Stochastic Gradient Descent

This is the ES Toolkit :

### CMA-ES (Covariance Matrix Adaptation Evolution Strategy)

```python
import cma
import numpy as np

# Objective function to be minimized
def objective_function(x):
    return np.sum(x**2)

# Initial guess
x0 = np.random.randn(10)

# Standard deviation
sigma = 0.5

# Run CMA-ES
es = cma.CMAEvolutionStrategy(x0, sigma)
es.optimize(objective_function)

# Best solution found
best_solution = es.result.xbest
```

### PEPG (Parameter Exploring Policy Gradients)

```python
import numpy as np

# Objective function to be maximized
def objective_function(x):
    return -np.sum(x**2)

# Initial guess
x0 = np.random.randn(10)

# Learning rate
alpha = 0.1

# Number of iterations
num_iterations = 1000

# Initialize parameters
theta = x0
sigma = 0.1

for i in range(num_iterations):
    # Sample perturbations
    epsilon = np.random.randn(*theta.shape)

    # Evaluate objective function
    reward_pos = objective_function(theta + sigma * epsilon)
    reward_neg = objective_function(theta - sigma * epsilon)

    # Update parameters
    theta += alpha / (2 * sigma) * (reward_pos - reward_neg) * epsilon

# Best solution found
best_solution = theta
```

### NES (Natural Evolution Strategy)

```python
import numpy as np

# Objective function to be maximized
def objective_function(x):
    return -np.sum(x**2)

# Initial guess
x0 = np.random.randn(10)

# Learning rate
alpha = 0.1

# Number of samples
num_samples = 50

# Number of iterations
num_iterations = 1000

# Initialize parameters
theta = x0
sigma = 0.1

for i in range(num_iterations):
    # Sample perturbations
    epsilon = np.random.randn(num_samples, *theta.shape)

    # Evaluate objective function
    rewards = np.array([objective_function(theta + sigma * eps) for eps in epsilon])

    # Compute gradient estimate
    grad_estimate = np.dot(rewards, epsilon) / (num_samples * sigma)

    # Update parameters
    theta += alpha * grad_estimate

# Best solution found
best_solution = theta
```

### OpenAI-ES (OpenAI Evolution Strategy)

```python
import numpy as np

# Objective function to be maximized
def objective_function(x):
    return -np.sum(x**2)

# Initial guess
x0 = np.random.randn(10)

# Learning rate
alpha = 0.1

# Number of samples
num_samples = 50

# Number of iterations
num_iterations = 1000

# Initialize parameters
theta = x0
sigma = 0.1

for i in range(num_iterations):
    # Sample perturbations
    epsilon = np.random.randn(num_samples, *theta.shape)

    # Evaluate objective function
    rewards = np.array([objective_function(theta + sigma * eps) for eps in epsilon])

    # Compute baseline
    baseline = np.mean(rewards)

    # Compute gradient estimate
    grad_estimate = np.dot(rewards - baseline, epsilon) / (num_samples * sigma)

    # Update parameters
    theta += alpha * grad_estimate

# Best solution found
best_solution = theta
```

### Simple ES (Simple Evolution Strategy)

```python
import numpy as np

# Objective function to be maximized
def objective_function(x):
    return -np.sum(x**2)

# Initial guess
x0 = np.random.randn(10)

# Learning rate
alpha = 0.1

# Number of samples
num_samples = 50

# Number of iterations
num_iterations = 1000

# Initialize parameters
theta = x0
sigma = 0.1

for i in range(num_iterations):
    # Sample perturbations
    epsilon = np.random.randn(num_samples, *theta.shape)

    # Evaluate objective function
    rewards = np.array([objective_function(theta + sigma * eps) for eps in epsilon])

    # Compute gradient estimate
    grad_estimate = np.dot(rewards, epsilon) / (num_samples * sigma)

    # Update parameters
    theta += alpha * grad_estimate

# Best solution found
best_solution = theta
```

# Additional Reading

[A Visual Guide to Evolution Strategies](https://blog.otoro.net/2017/10/29/visual-evolution-strategies/)
