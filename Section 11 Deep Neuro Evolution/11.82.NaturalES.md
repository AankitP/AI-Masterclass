# Parameter Exploring Policy Gradients (PEPG-ES)

Differences between PEPG and others:
- PEPG allows for far more parameters
- instead of taking only the best solutions, we will keep even the worst solutions
    - this is because weak solutions also contain information on what not to do
        - Weak solutions tell the NN where not to go when finding the optimal solution
- Classic gradient is used to update the parameters
    - meaning that backpropagation comes back

instead of using the typical loss error function, but we will use the expected value of the fitness function

