# The Whole Implementation

This section will cover the Final Run of the CNN-VAE,MDN-RNN Full World Model

# This is the Game code:

```python

# Building the Environment

# Importing the necessary libraries
import numpy as np  # For numerical operations
from scipy.misc import imresize as resize  # For resizing images (deprecated, consider using alternatives like PIL or OpenCV)
from gym.spaces.box import Box  # For defining the observation space
from gym.envs.box2d.car_racing import CarRacing  # Importing the CarRacing environment from OpenAI Gym

# Setting the dimensions of the game screen
SCREEN_X = 64  # Width of the resized game screen
SCREEN_Y = 64  # Height of the resized game screen

# Function to preprocess and resize the game frame to 64x64 dimensions
def _process_frame(frame):
    # Normalize pixel values to the range [0, 1]
    obs = frame[0:84, :, :].astype(np.float) / 255.0
    # Resize the frame to 64x64 dimensions
    obs = resize(obs, (64, 64))
    # Invert the colors and convert back to uint8 format
    obs = ((1.0 - obs) * 255).round().astype(np.uint8)
    return obs

# Class to create a custom CarRacing environment with modified behavior
class CarRacingWrapper(CarRacing):
    # Initialize the CarRacingWrapper class
    def __init__(self, full_episode=False):
        super(CarRacingWrapper, self).__init__()  # Call the parent class constructor
        self.full_episode = full_episode  # Whether to ignore the 'done' signal
        # Define the observation space with the resized screen dimensions
        self.observation_space = Box(low=0, high=255, shape=(SCREEN_X, SCREEN_Y, 3))

    # Override the step function to modify the behavior of the environment
    def _step(self, action):
        # Perform the action and get the next state, reward, and done signal
        obs, reward, done, _ = super(CarRacingWrapper, self)._step(action)
        # If full_episode is True, ignore the 'done' signal and always return False
        if self.full_episode:
            return _process_frame(obs), reward, False, {}
        # Otherwise, return the processed frame, reward, and the original 'done' signal
        return _process_frame(obs), reward, done, {}

# Function to create and return a CarRacing environment
def make_env(env_name, seed=-1, render_mode=False, full_episode=False):
    # Create an instance of the CarRacingWrapper class
    env = CarRacingWrapper(full_episode=full_episode)
    # Set the random seed for reproducibility if a valid seed is provided
    if seed >= 0:
        env.seed(seed)
    return env

# Function to run the game with keyboard controls
def game_runner():
    from pyglet.window import key  # Import the key module for handling keyboard input
    a = np.array([0.0, 0.0, 0.0])  # Initialize the action array (steering, gas, brake)

    # Function to handle key press events
    def key_press(k, mod):
        global restart  # Declare the restart variable as global
        if k == 0xff0d: restart = True  # Restart the game if the Enter key is pressed
        if k == key.LEFT:  a[0] = -1.0  # Steer left
        if k == key.RIGHT: a[0] = +1.0  # Steer right
        if k == key.UP:    a[1] = +1.0  # Accelerate
        if k == key.DOWN:  a[2] = +0.8  # Brake

    # Function to handle key release events
    def key_release(k, mod):
        if k == key.LEFT and a[0] == -1.0: a[0] = 0  # Stop steering left
        if k == key.RIGHT and a[0] == +1.0: a[0] = 0  # Stop steering right
        if k == key.UP:    a[1] = 0  # Stop accelerating
        if k == key.DOWN:  a[2] = 0  # Stop braking

    # Create an instance of the CarRacing environment
    env = CarRacing()
    env.render()  # Render the game window
    # Set the key press and release event handlers
    env.viewer.window.on_key_press = key_press
    env.viewer.window.on_key_release = key_release

    # Main game loop
    while True:
        env.reset()  # Reset the environment to the initial state
        total_reward = 0.0  # Initialize the total reward
        steps = 0  # Initialize the step counter
        restart = False  # Initialize the restart flag

        # Loop to play the game until the episode ends or the user restarts
        while True:
            s, r, done, info = env.step(a)  # Perform the action and get the next state, reward, and done signal
            total_reward += r  # Accumulate the reward
            # If the maximum number of steps is reached, print the results and break
            if steps == 900:
                print("\n")
                print("_______________________________")
                print("\n")
                print("Human Intelligence Result:")
                print("Total Steps: {}".format(steps))
                print("Total Reward: {:.0f}".format(total_reward))
                print("\n")
                print("_______________________________")
                print("\n")
                break
            steps += 1  # Increment the step counter
            env.render()  # Render the game window
            if restart: break  # Break the loop if the user restarts the game
    env.monitor.close()  # Close the environment monitor

# Run the game if the script is executed directly
if __name__ == "__main__":
    game_runner()
```

# CNN-VAE

```python
# Building the CNN-VAE model

# Importing the libraries

import numpy as np
import tensorflow as tf
import json
import os

# Building the CNN-VAE model within a class

class ConvVAE(object):

  # Initializing all the parameters and variables of the ConvVAE class
  def __init__(self, z_size=32, batch_size=1, learning_rate=0.0001, kl_tolerance=0.5, is_training=False, reuse=False, gpu_mode=False):
    self.z_size = z_size
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    self.kl_tolerance = kl_tolerance
    self.is_training = is_training
    self.reuse = reuse
    with tf.variable_scope('conv_vae', reuse=self.reuse):
      if not gpu_mode:
        with tf.device('/cpu:0'):
          tf.logging.info('Model using cpu.')
          self._build_graph()
      else:
        tf.logging.info('Model using gpu.')
        self._build_graph()
    self._init_session()

  # Making a method that creates the VAE model architecture itself
  def _build_graph(self):
    self.g = tf.Graph()
    with self.g.as_default():
      self.x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])
      # Building the Encoder part of the VAE
      h = tf.layers.conv2d(self.x, 32, 4, strides=2, activation=tf.nn.relu, name="enc_conv1")
      h = tf.layers.conv2d(h, 64, 4, strides=2, activation=tf.nn.relu, name="enc_conv2")
      h = tf.layers.conv2d(h, 128, 4, strides=2, activation=tf.nn.relu, name="enc_conv3")
      h = tf.layers.conv2d(h, 256, 4, strides=2, activation=tf.nn.relu, name="enc_conv4")
      h = tf.reshape(h, [-1, 2*2*256])
      # Building the "V" part of the VAE
      self.mu = tf.layers.dense(h, self.z_size, name="enc_fc_mu")
      self.logvar = tf.layers.dense(h, self.z_size, name="enc_fc_log_var")
      self.sigma = tf.exp(self.logvar / 2.0)
      self.epsilon = tf.random_normal([self.batch_size, self.z_size])
      self.z = self.mu + self.sigma * self.epsilon
      # Building the Decoder part of the VAE
      h = tf.layers.dense(self.z, 1024, name="dec_fc")
      h = tf.reshape(h, [-1, 1, 1, 1024])
      h = tf.layers.conv2d_transpose(h, 128, 5, strides=2, activation=tf.nn.relu, name="dec_deconv1")
      h = tf.layers.conv2d_transpose(h, 64, 5, strides=2, activation=tf.nn.relu, name="dec_deconv2")
      h = tf.layers.conv2d_transpose(h, 32, 6, strides=2, activation=tf.nn.relu, name="dec_deconv3")
      self.y = tf.layers.conv2d_transpose(h, 3, 6, strides=2, activation=tf.nn.sigmoid, name="dec_deconv4")
      # Implementing the training operations
      if self.is_training:
        self.global_step = tf.Variable(0, name='global_step', trainable=False)
        self.r_loss = tf.reduce_sum(tf.square(self.x - self.y), reduction_indices = [1,2,3])
        self.r_loss = tf.reduce_mean(self.r_loss)
        self.kl_loss = - 0.5 * tf.reduce_sum((1 + self.logvar - tf.square(self.mu) - tf.exp(self.logvar)), reduction_indices = 1)
        self.kl_loss = tf.maximum(self.kl_loss, self.kl_tolerance * self.z_size)
        self.kl_loss = tf.reduce_mean(self.kl_loss)
        self.loss = self.r_loss + self.kl_loss
        self.lr = tf.Variable(self.learning_rate, trainable=False)
        self.optimizer = tf.train.AdamOptimizer(self.lr)
        grads = self.optimizer.compute_gradients(self.loss)
        self.train_op = self.optimizer.apply_gradients(grads, global_step=self.global_step, name='train_step')
      self.init = tf.global_variables_initializer()

  # Making a method that initializes a TensorFlow session
  def _init_session(self):
    self.sess = tf.Session(graph=self.g)
    self.sess.run(self.init)

  # Making a method that closes a TensorFlow session
  def close_sess(self):
    self.sess.close()

  # Making a method that encodes a raw frame x into the latent space representation
  def encode(self, x):
    return self.sess.run(self.z, feed_dict={self.x: x})

  # Making a method that encodes a raw frame x into the mean and logvariance parts of the latent vectors space
  def encode_mu_logvar(self, x):
    (mu, logvar) = self.sess.run([self.mu, self.logvar], feed_dict={self.x: x})
    return mu, logvar

  # Making a method that decodes a latent vector z into the reconstructed frame
  def decode(self, z):
    return self.sess.run(self.y, feed_dict={self.z: z})

  # Making a method that gets the training parameters of the VAE model
  def get_model_params(self):
    model_names = []
    model_params = []
    model_shapes = []
    with self.g.as_default():
      t_vars = tf.trainable_variables()
      for var in t_vars:
        param_name = var.name
        p = self.sess.run(var)
        model_names.append(param_name)
        params = np.round(p*10000).astype(np.int).tolist()
        model_params.append(params)
        model_shapes.append(p.shape)
    return model_params, model_shapes, model_names

  # Making a method that gets the random parameters of the VAE model
  def get_random_model_params(self, stdev=0.5):
    _, mshape, _ = self.get_model_params()
    rparam = []
    for s in mshape:
      rparam.append(np.random.standard_cauchy(s)*stdev)
    return rparam

  # Making a method that sets specific weights to chosen values in the VAE model
  def set_model_params(self, params):
    with self.g.as_default():
      t_vars = tf.trainable_variables()
      idx = 0
      for var in t_vars:
        pshape = self.sess.run(var).shape
        p = np.array(params[idx])
        assert pshape == p.shape, "inconsistent shape"
        assign_op = var.assign(p.astype(np.float)/10000.)
        self.sess.run(assign_op)
        idx += 1

  # Making a method that loads saved VAE weights from a JSON file
  def load_json(self, jsonfile='Weights/vae_weights.json'):
    with open(jsonfile, 'r') as f:
      params = json.load(f)
    self.set_model_params(params)

  # Making a method that saves trained VAE weights into a JSON file
  def save_json(self, jsonfile='Weights/vae_weights.json'):
    model_params, model_shapes, model_names = self.get_model_params()
    qparams = []
    for p in model_params:
      qparams.append(p)
    with open(jsonfile, 'wt') as outfile:
      json.dump(qparams, outfile, sort_keys=True, indent=0, separators=(',', ': '))

  # Making a method that sets some parameters to random values in the VAE model (this is usually done at the beginning of the training process)
  def set_random_params(self, stdev=0.5):
    rparam = self.get_random_model_params(stdev)
    self.set_model_params(rparam)

  # Making a method that saves the model into a chosen directory
  def save_model(self, model_save_path):
    sess = self.sess
    with self.g.as_default():
      saver = tf.train.Saver(tf.global_variables())
    checkpoint_path = os.path.join(model_save_path, 'vae')
    tf.logging.info('saving model %s.', checkpoint_path)
    saver.save(sess, checkpoint_path, 0)

  # Making a method that loads a saved checkpoint that restores all saved trained VAE weights
  def load_checkpoint(self, checkpoint_path):
    sess = self.sess
    with self.g.as_default():
      saver = tf.train.Saver(tf.global_variables())
    ckpt = tf.train.get_checkpoint_state(checkpoint_path)
    print('loading model', ckpt.model_checkpoint_path)
    tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)
    saver.restore(sess, ckpt.model_checkpoint_path)
```

# MDN-RNN

```python
# Building the MDN-RNN model

# Importing the libraries

import numpy as np
import tensorflow as tf
import json
from collections import namedtuple

# Building the MDN-RNN model within a class

class MDNRNN(object):

  # Initializing all the parameters and variables of the MDNRNN class
  def __init__(self, hps, reuse=False, gpu_mode=False):
    self.hps = hps
    with tf.variable_scope('mdn_rnn', reuse=reuse):
      if not gpu_mode:
        with tf.device('/cpu:0'):
          tf.logging.info('Model using cpu.')
          self.g = tf.Graph()
          with self.g.as_default():
            self.build_model(hps)
      else:
        tf.logging.info('Model using gpu.')
        self.g = tf.Graph()
        with self.g.as_default():
          self.build_model(hps)
    self._init_session()

  # Making a method that creates the MDN-RNN model architecture itself
  def build_model(self, hps):
    # Building the RNN
    self.num_mixture = hps.num_mixture
    KMIX = self.num_mixture
    INWIDTH = hps.input_seq_width
    OUTWIDTH = hps.output_seq_width
    LENGTH = self.hps.max_seq_len
    if hps.is_training:
      self.global_step = tf.Variable(0, name='global_step', trainable=False)
    cell_fn = tf.contrib.rnn.LayerNormBasicLSTMCell
    use_recurrent_dropout = False if self.hps.use_recurrent_dropout == 0 else True
    use_input_dropout = False if self.hps.use_input_dropout == 0 else True
    use_output_dropout = False if self.hps.use_output_dropout == 0 else True
    use_layer_norm = False if self.hps.use_layer_norm == 0 else True
    if use_recurrent_dropout:
      cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm, dropout_keep_prob=self.hps.recurrent_dropout_prob)
    else:
      cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm)
    if use_input_dropout:
      cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=self.hps.input_dropout_prob)
    if use_output_dropout:
      cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.hps.output_dropout_prob)
    self.cell = cell
    self.sequence_lengths = LENGTH
    self.input_x = tf.placeholder(dtype=tf.float32, shape=[self.hps.batch_size, self.hps.max_seq_len, INWIDTH])
    self.output_x = tf.placeholder(dtype=tf.float32, shape=[self.hps.batch_size, self.hps.max_seq_len, OUTWIDTH])
    actual_input_x = self.input_x
    self.initial_state = cell.zero_state(batch_size=hps.batch_size, dtype=tf.float32)
    NOUT = OUTWIDTH * KMIX * 3
    with tf.variable_scope('RNN'):
      output_w = tf.get_variable("output_w", [self.hps.rnn_size, NOUT])
      output_b = tf.get_variable("output_b", [NOUT])
    output, last_state = tf.nn.dynamic_rnn(cell,
                                           actual_input_x,
                                           initial_state=self.initial_state,
                                           time_major=False,
                                           swap_memory=True,
                                           dtype=tf.float32,
                                           scope="RNN")
    # Building the MDN
    output = tf.reshape(output, [-1, hps.rnn_size])
    output = tf.nn.xw_plus_b(output, output_w, output_b)
    output = tf.reshape(output, [-1, KMIX * 3])
    self.final_state = last_state
    def get_mdn_coef(output):
      logmix, mean, logstd = tf.split(output, 3, 1)
      logmix = logmix - tf.reduce_logsumexp(logmix, 1, keepdims=True)
      return logmix, mean, logstd
    out_logmix, out_mean, out_logstd = get_mdn_coef(output)
    self.out_logmix = out_logmix
    self.out_mean = out_mean
    self.out_logstd = out_logstd
    # Implementing the training operations
    logSqrtTwoPI = np.log(np.sqrt(2.0 * np.pi))
    def tf_lognormal(y, mean, logstd):
      return -0.5 * ((y - mean) / tf.exp(logstd)) ** 2 - logstd - logSqrtTwoPI
    def get_lossfunc(logmix, mean, logstd, y):
      v = logmix + tf_lognormal(y, mean, logstd)
      v = tf.reduce_logsumexp(v, 1, keepdims=True)
      return -tf.reduce_mean(v)
    flat_target_data = tf.reshape(self.output_x,[-1, 1])
    lossfunc = get_lossfunc(out_logmix, out_mean, out_logstd, flat_target_data)
    self.cost = tf.reduce_mean(lossfunc)
    if self.hps.is_training == 1:
      self.lr = tf.Variable(self.hps.learning_rate, trainable=False)
      self.optimizer = tf.train.AdamOptimizer(self.lr)
      gvs = self.optimizer.compute_gradients(self.cost)
      capped_gvs = [(tf.clip_by_value(grad, -self.hps.grad_clip, self.hps.grad_clip), var) for grad, var in gvs]
      self.train_op = self.optimizer.apply_gradients(capped_gvs, global_step=self.global_step, name='train_step')
    self.init = tf.global_variables_initializer()

  # Making a method that initializes the tensorflow graph session, used to run the MDN-RNN model inference or training
  def _init_session(self):
    self.sess = tf.Session(graph=self.g)
    self.sess.run(self.init)

  # Making a method that closes the tensorflow graph session currently running (closing a session is necessary to overcome nested graphs)
  def close_sess(self):
    self.sess.close()

  # Making a method that extracts all trainable variables from the RNN graph into a python list
  def get_model_params(self):
    model_names = []
    model_params = []
    model_shapes = []
    with self.g.as_default():
      t_vars = tf.trainable_variables()
      for var in t_vars:
        param_name = var.name
        p = self.sess.run(var)
        model_names.append(param_name)
        params = np.round(p*10000).astype(np.int).tolist()
        model_params.append(params)
        model_shapes.append(p.shape)
    return model_params, model_shapes, model_names

  # Making a method that randomly initializes the RNN parameters
  def get_random_model_params(self, stdev=0.5):
    _, mshape, _ = self.get_model_params()
    rparam = []
    for s in mshape:
      rparam.append(np.random.standard_cauchy(s)*stdev)
    return rparam

  # Making a method that sets some parameters to random values in the RNN model (this is usually done at the beginning of the training process)
  def set_random_params(self, stdev=0.5):
    rparam = self.get_random_model_params(stdev)
    self.set_model_params(rparam)

  # Making a method that sets specific weights to chosen values in the RNN model
  def set_model_params(self, params):
    with self.g.as_default():
      t_vars = tf.trainable_variables()
      idx = 0
      for var in t_vars:
        pshape = self.sess.run(var).shape
        p = np.array(params[idx])
        assert pshape == p.shape, "inconsistent shape"
        assign_op = var.assign(p.astype(np.float)/10000.)
        self.sess.run(assign_op)
        idx += 1

  # Making a method that loads saved RNN weights from a JSON file
  def load_json(self, jsonfile='Weights/rnn_weights.json'):
    with open(jsonfile, 'r') as f:
      params = json.load(f)
    self.set_model_params(params)

  # Making a method that saves trained RNN weights into a JSON file
  def save_json(self, jsonfile='Weights/rnn_weights.json'):
    model_params, model_shapes, model_names = self.get_model_params()
    qparams = []
    for p in model_params:
      qparams.append(p)
    with open(jsonfile, 'wt') as outfile:
      json.dump(qparams, outfile, sort_keys=True, indent=0, separators=(',', ': '))

# Setting the Hyperparameters

MODE_ZCH = 0
MODE_ZC = 1
MODE_Z = 2
MODE_Z_HIDDEN = 3
MODE_ZH = 4
HyperParams = namedtuple('HyperParams', ['num_steps',
                                         'max_seq_len',
                                         'input_seq_width',
                                         'output_seq_width',
                                         'rnn_size',
                                         'batch_size',
                                         'grad_clip',
                                         'num_mixture',
                                         'learning_rate',
                                         'decay_rate',
                                         'min_learning_rate',
                                         'use_layer_norm',
                                         'use_recurrent_dropout',
                                         'recurrent_dropout_prob',
                                         'use_input_dropout',
                                         'input_dropout_prob',
                                         'use_output_dropout',
                                         'output_dropout_prob',
                                         'is_training',
                                        ])

# Making a function that returns all the default hyperparameters of the MDN-RNN model

def default_hps():
  return HyperParams(num_steps=2000,
                     max_seq_len=1000,
                     input_seq_width=35,
                     output_seq_width=32,
                     rnn_size=256,
                     batch_size=100,
                     grad_clip=1.0,
                     num_mixture=5,
                     learning_rate=0.001,
                     decay_rate=1.0,
                     min_learning_rate=0.00001,
                     use_layer_norm=0,
                     use_recurrent_dropout=0,
                     recurrent_dropout_prob=0.90,
                     use_input_dropout=0,
                     input_dropout_prob=0.90,
                     use_output_dropout=0,
                     output_dropout_prob=0.90,
                     is_training=1)

# Getting and sampling these default hyperparameters

hps_model = default_hps()
hps_sample = hps_model._replace(batch_size=1, max_seq_len=1, use_recurrent_dropout=0, is_training=0)

# Making a function that samples the index of a probability distribution function (pdf)
def get_pi_idx(x, pdf):
  N = pdf.size
  accumulate = 0
  for i in range(0, N):
    accumulate += pdf[i]
    if (accumulate >= x):
      return i
  print('error with sampling ensemble')
  return -1

# Making a function that samples sequences of inputs for the RNN model using a pre-trained VAE model

def sample_sequence(sess, s_model, hps, init_z, actions, temperature=1.0, seq_len=1000):
  OUTWIDTH = hps.output_seq_width
  prev_x = np.zeros((1, 1, OUTWIDTH))
  prev_x[0][0] = init_z
  prev_state = sess.run(s_model.initial_state)
  strokes = np.zeros((seq_len, OUTWIDTH), dtype=np.float32)
  for i in range(seq_len):
    input_x = np.concatenate((prev_x, actions[i].reshape((1, 1, 3))), axis=2)
    feed = {s_model.input_x: input_x, s_model.initial_state:prev_state}
    [logmix, mean, logstd, next_state] = sess.run([s_model.out_logmix, s_model.out_mean, s_model.out_logstd, s_model.final_state], feed)
    logmix2 = np.copy(logmix)/temperature
    logmix2 -= logmix2.max()
    logmix2 = np.exp(logmix2)
    logmix2 /= logmix2.sum(axis=1).reshape(OUTWIDTH, 1)
    mixture_idx = np.zeros(OUTWIDTH)
    chosen_mean = np.zeros(OUTWIDTH)
    chosen_logstd = np.zeros(OUTWIDTH)
    for j in range(OUTWIDTH):
      idx = get_pi_idx(np.random.rand(), logmix2[j])
      mixture_idx[j] = idx
      chosen_mean[j] = mean[j][idx]
      chosen_logstd[j] = logstd[j][idx]
    rand_gaussian = np.random.randn(OUTWIDTH)*np.sqrt(temperature)
    next_x = chosen_mean+np.exp(chosen_logstd)*rand_gaussian
    strokes[i,:] = next_x
    prev_x[0][0] = next_x
    prev_state = next_state
  return strokes

# Making a function that returns the initial state of the RNN model

def rnn_init_state(rnn):
  return rnn.sess.run(rnn.initial_state)

# Making a function that returns the final state of the RNN model

def rnn_next_state(rnn, z, a, prev_state):
  input_x = np.concatenate((z.reshape((1, 1, 32)), a.reshape((1, 1, 3))), axis=2)
  feed = {rnn.input_x: input_x, rnn.initial_state:prev_state}
  return rnn.sess.run(rnn.final_state, feed)

# Making a function that returns the size of the RNN output depending on the mode

def rnn_output_size(mode):
  if mode == MODE_ZCH:
    return (32+256+256)
  if (mode == MODE_ZC) or (mode == MODE_ZH):
    return (32+256)
  return 32

# Making a function that returns the RNN output depending on the mode

def rnn_output(state, z, mode):
  if mode == MODE_ZCH:
    return np.concatenate([z, np.concatenate((state.c,state.h), axis=1)[0]])
  if mode == MODE_ZC:
    return np.concatenate([z, state.c[0]])
  if mode == MODE_ZH:
    return np.concatenate([z, state.h[0]])
  return z

```

# Full World Model

```python

# Running the whole Full World Model

# Importing the libraries

import os
import numpy as np
import random
import json
from env import make_env
from vae import ConvVAE
from rnn import hps_sample, MDNRNN, rnn_init_state, rnn_next_state, rnn_output, rnn_output_size

# Setting the Hyperparameters

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
render_mode = True
MODE_ZCH = 0
MODE_ZC = 1
MODE_Z = 2
MODE_Z_HIDDEN = 3
MODE_ZH = 4
EXP_MODE = MODE_ZH

# Making a function that loads and returns the model

def make_model(load_model=True):
  model = Model(load_model=load_model)
  return model

# Making a clipping function that will later be used to clip the actions

def clip(x, lo=0.0, hi=1.0):
  return np.minimum(np.maximum(x, lo), hi)

# Assembling the whole Full World Model within a class

class Model:

  # Initializing all the parameters and variables of the Model class
  def __init__(self, load_model=True):
    self.env_name = "carracing"
    self.vae = ConvVAE(batch_size=1, gpu_mode=False, is_training=False, reuse=True)
    self.rnn = MDNRNN(hps_sample, gpu_mode=False, reuse=True)
    if load_model:
      self.vae.load_json('Weights/vae_weights.json')
      self.rnn.load_json('Weights/rnn_weights.json')
    self.state = rnn_init_state(self.rnn)
    self.rnn_mode = True
    self.input_size = rnn_output_size(EXP_MODE)
    self.z_size = 32
    if EXP_MODE == MODE_Z_HIDDEN:
      self.hidden_size = 40
      self.weight_hidden = np.random.randn(self.input_size, self.hidden_size)
      self.bias_hidden = np.random.randn(self.hidden_size)
      self.weight_output = np.random.randn(self.hidden_size, 3)
      self.bias_output = np.random.randn(3)
      self.param_count = ((self.input_size+1)*self.hidden_size) + (self.hidden_size*3+3)
    else:
      self.weight = np.random.randn(self.input_size, 3)
      self.bias = np.random.randn(3)
      self.param_count = (self.input_size)*3+3
    self.render_mode = False

  # Making a method that creates an environment (in our case the CarRacing game) inside which both the AI and HI will play
  def make_env(self, seed=-1, render_mode=False, full_episode=False):
    self.render_mode = render_mode
    self.env = make_env(self.env_name, seed=seed, render_mode=render_mode, full_episode=full_episode)

  # Making a method that reinitiates the states for the RNN model
  def reset(self):
    self.state = rnn_init_state(self.rnn)

  # Making a method that encodes the observations (input frames)
  def encode_obs(self, obs):
    result = np.copy(obs).astype(np.float)/255.0
    result = result.reshape(1, 64, 64, 3)
    mu, logvar = self.vae.encode_mu_logvar(result)
    mu = mu[0]
    logvar = logvar[0]
    s = logvar.shape
    z = mu + np.exp(logvar/2.0) * np.random.randn(*s)
    return z, mu, logvar

  # Making a method that samples an action based on the latent vector z
  def get_action(self, z):
    h = rnn_output(self.state, z, EXP_MODE)
    if EXP_MODE == MODE_Z_HIDDEN:
      h = np.tanh(np.dot(h, self.weight_hidden) + self.bias_hidden)
      action = np.tanh(np.dot(h, self.weight_output) + self.bias_output)
    else:
      action = np.tanh(np.dot(h, self.weight) + self.bias)
    action[1] = (action[1]+1.0) / 2.0
    action[2] = clip(action[2])
    self.state = rnn_next_state(self.rnn, z, action, self.state)
    return action

  # Making a method that sets the initialized/loaded weights into the model
  def set_model_params(self, model_params):
    if EXP_MODE == MODE_Z_HIDDEN:
      params = np.array(model_params)
      cut_off = (self.input_size+1)*self.hidden_size
      params_1 = params[:cut_off]
      params_2 = params[cut_off:]
      self.bias_hidden = params_1[:self.hidden_size]
      self.weight_hidden = params_1[self.hidden_size:].reshape(self.input_size, self.hidden_size)
      self.bias_output = params_2[:3]
      self.weight_output = params_2[3:].reshape(self.hidden_size, 3)
    else:
      self.bias = np.array(model_params[:3])
      self.weight = np.array(model_params[3:]).reshape(self.input_size, 3)

  # Making a method that loads the model weights
  def load_model(self, filename):
    with open(filename) as f:
      data = json.load(f)
    print('loading file %s' % (filename))
    self.data = data
    model_params = np.array(data[0])
    self.set_model_params(model_params)

  # Making a method that randomly initializes the weights
  def get_random_model_params(self, stdev=0.1):
    return np.random.standard_cauchy(self.param_count)*stdev

  # Making a method that randomly initializes the weights for all 3 parts of the Full World Model (VAE, RNN, Controller)
  def init_random_model_params(self, stdev=0.1):
    params = self.get_random_model_params(stdev=stdev)
    self.set_model_params(params)
    vae_params = self.vae.get_random_model_params(stdev=stdev)
    self.vae.set_model_params(vae_params)
    rnn_params = self.rnn.get_random_model_params(stdev=stdev)
    self.rnn.set_model_params(rnn_params)

# Making a function that runs a 1000-steps simulation of the Full World Model inside the CarRacing environment

def simulate(model, train_mode=False, render_mode=True, num_episode=5, seed=-1, max_len=-1):
  reward_list = []
  t_list = []
  max_episode_length = 900
  recording_mode = False
  penalize_turning = False
  if train_mode and max_len > 0:
    max_episode_length = max_len
  if (seed >= 0):
    random.seed(seed)
    np.random.seed(seed)
    model.env.seed(seed)
  for episode in range(num_episode):
    model.reset()
    obs = model.env.reset()
    total_reward = 0.0
    random_generated_int = np.random.randint(2**31-1)
    filename = "record/"+str(random_generated_int)+".npz"
    recording_mu = []
    recording_logvar = []
    recording_action = []
    recording_reward = [0]
    for t in range(max_episode_length+1):
      if render_mode:
        model.env.render("human")
      else:
        model.env.render('rgb_array')
      z, mu, logvar = model.encode_obs(obs)
      action = model.get_action(z)
      recording_mu.append(mu)
      recording_logvar.append(logvar)
      recording_action.append(action)
      obs, reward, done, info = model.env.step(action)
      extra_reward = 0.0
      if train_mode and penalize_turning:
        extra_reward -= np.abs(action[0])/10.0
        reward += extra_reward
      recording_reward.append(reward)
      total_reward += reward
    z, mu, logvar = model.encode_obs(obs)
    action = model.get_action(z)
    recording_mu.append(mu)
    recording_logvar.append(logvar)
    recording_action.append(action)
    recording_mu = np.array(recording_mu, dtype=np.float16)
    recording_logvar = np.array(recording_logvar, dtype=np.float16)
    recording_action = np.array(recording_action, dtype=np.float16)
    recording_reward = np.array(recording_reward, dtype=np.float16)
    if not render_mode:
      if recording_mode:
        np.savez_compressed(filename, mu=recording_mu, logvar=recording_logvar, action=recording_action, reward=recording_reward)
    if render_mode:
      print("\n")
      print("_______________________________")
      print("\n")
      print("Artificial Intelligence Result:")
      print("Total Steps: {}".format(t))
      print("Total Reward: {:.0f}".format(total_reward))
      print("\n")
      print("_______________________________")
      print("\n")
    reward_list.append(total_reward)
    t_list.append(t)
  return reward_list, t_list

# Implementing the main code to run the competition between AI and HI at the CarRacing game

def main():

  render_mode_string = "render"
  if (render_mode_string == "render"):
    render_mode = True
  else:
    render_mode = False
  use_model = False
  use_model = True
  filename = "Weights/controller_weights.json"
  if (use_model):
    model = make_model()
    model.make_env(render_mode=render_mode)
    model.load_model(filename)
  else:
    model = make_model(load_model=False)
    model.make_env(render_mode=render_mode)
    model.init_random_model_params(stdev=np.random.rand()*0.01)
  N_episode = 100
  if render_mode:
    N_episode = 1
  reward_list = []
  for i in range(N_episode):
    reward, steps_taken = simulate(model, train_mode=False, render_mode=render_mode, num_episode=1)
    reward_list.append(reward[0])

if __name__ == "__main__":
  from multiprocessing import Process
  from env import game_runner
  p1 = Process(target=main())
  p1.start()
  p2 = Process(target=game_runner())
  p2.start()
  p1.join()
  p2.join()


```
